{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SINGLE-CELL PONG DQN + DOUBLE DQN + TUNED DOUBLE DQN\n",
        "# -------------------------------------------------------------------------------\n",
        "# 1) Colab: Runtime -> Change runtime type -> Hardware accelerator: GPU\n",
        "# 2) Run this cell.\n",
        "\n",
        "!pip install -q gymnasium ale-py \"autorom[accept-rom-license]\" imageio imageio-ffmpeg\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordEpisodeStatistics, AtariPreprocessing\n",
        "\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "ENV_ID = \"PongNoFrameskip-v4\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 50_000\n",
        "REPLAY_START_SIZE = 10_000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1_000\n",
        "\n",
        "EPS_START = 1.0\n",
        "EPS_FINAL = 0.02\n",
        "EPS_DECAY_FRAMES_BASE = 1_000_000\n",
        "\n",
        "MAX_FRAMES_DQN_BASE = 400_000\n",
        "MAX_FRAMES_DDQN_BASE = 400_000\n",
        "\n",
        "MAX_FRAMES_DDQN_TUNED = 600_000\n",
        "EPS_DECAY_FRAMES_TUNED = 300_000\n",
        "\n",
        "VIDEO_MAX_STEPS = 5_000\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    run_name: str\n",
        "    variant_name: str\n",
        "    use_ddqn: bool\n",
        "    max_frames: int\n",
        "    eps_decay_frames: int\n",
        "    comment: str = \"\"\n",
        "\n",
        "experiment_logs: List[Dict[str, Any]] = []\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, num_stack):\n",
        "        super().__init__(env)\n",
        "        self.num_stack = num_stack\n",
        "        self.frames = deque(maxlen=num_stack)\n",
        "        old_space = env.observation_space\n",
        "        low = np.repeat(old_space.low[np.newaxis, ...], num_stack, axis=0)\n",
        "        high = np.repeat(old_space.high[np.newaxis, ...], num_stack, axis=0)\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=old_space.dtype)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        for _ in range(self.num_stack):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_obs(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_obs(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.stack(self.frames, axis=0)\n",
        "\n",
        "def make_env(env_id: str) -> gym.Env:\n",
        "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "    env = AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, grayscale_obs=True, scale_obs=False)\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "Experience = namedtuple(\"Experience\", [\"state\", \"action\", \"reward\", \"done\", \"next_state\"])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Experience(*args))\n",
        "\n",
        "    def sample(self, batch_size: int) -> List[Experience]:\n",
        "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        return [self.buffer[i] for i in idx]\n",
        "\n",
        "def batch_to_tensors(batch: List[Experience], device: torch.device):\n",
        "    states = np.stack([e.state for e in batch])\n",
        "    next_states = np.stack([e.next_state for e in batch])\n",
        "    actions = np.array([e.action for e in batch], dtype=np.int64)\n",
        "    rewards = np.array([e.reward for e in batch], dtype=np.float32)\n",
        "    dones = np.array([e.done for e in batch], dtype=np.bool_)\n",
        "\n",
        "    return (\n",
        "        torch.as_tensor(states, device=device),\n",
        "        torch.as_tensor(actions, device=device),\n",
        "        torch.as_tensor(rewards, device=device),\n",
        "        torch.as_tensor(dones, device=device),\n",
        "        torch.as_tensor(next_states, device=device),\n",
        "    )\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape: Tuple[int, int, int], n_actions: int):\n",
        "        super().__init__()\n",
        "        c, h, w = input_shape\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            o = self.conv(torch.zeros(1, c, h, w))\n",
        "            conv_size = o.view(1, -1).size(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255.0\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, buffer: ReplayBuffer):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device, epsilon: float):\n",
        "        if random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            q = net(torch.as_tensor(self.state, device=device).unsqueeze(0))\n",
        "            action = int(q.argmax(dim=1).item())\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        self.total_reward += reward\n",
        "        self.buffer.push(self.state, action, reward, done, next_state)\n",
        "        self.state = next_state\n",
        "\n",
        "        if done:\n",
        "            r = self.total_reward\n",
        "            self.reset()\n",
        "            return r\n",
        "        return None\n",
        "\n",
        "def calc_loss(batch, net, tgt_net, device, use_ddqn):\n",
        "    states_t, actions_t, rewards_t, dones_t, next_states_t = batch_to_tensors(batch, device)\n",
        "    q_sa = net(states_t).gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if use_ddqn:\n",
        "            next_actions = net(next_states_t).argmax(dim=1)\n",
        "            next_q = tgt_net(next_states_t).gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
        "        else:\n",
        "            next_q = tgt_net(next_states_t).max(dim=1)[0]\n",
        "\n",
        "        next_q[dones_t] = 0.0\n",
        "        expected = rewards_t + GAMMA * next_q\n",
        "\n",
        "    return nn.MSELoss()(q_sa, expected)\n",
        "\n",
        "def train_variant(config: ExperimentConfig):\n",
        "    print(f\"\\n=== Starting run: {config.run_name} ({config.variant_name}) ===\")\n",
        "    if config.comment:\n",
        "        print(\"  note:\", config.comment)\n",
        "\n",
        "    env = make_env(ENV_ID)\n",
        "    n_actions = env.action_space.n\n",
        "    obs_shape = env.observation_space.shape\n",
        "\n",
        "    buffer = ReplayBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "\n",
        "    net = DQN(obs_shape, n_actions).to(DEVICE)\n",
        "    tgt_net = DQN(obs_shape, n_actions).to(DEVICE)\n",
        "    tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    frame_idx = 0\n",
        "    epsilon = EPS_START\n",
        "    rewards_history = []\n",
        "    mean_rewards = []\n",
        "\n",
        "    while frame_idx < config.max_frames:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPS_FINAL, EPS_START - frame_idx / config.eps_decay_frames)\n",
        "\n",
        "        reward = agent.play_step(net, DEVICE, epsilon)\n",
        "        if reward is not None:\n",
        "            rewards_history.append(reward)\n",
        "            mean_r = np.mean(rewards_history[-100:])\n",
        "            mean_rewards.append((frame_idx, mean_r))\n",
        "            if len(rewards_history) % 10 == 0:\n",
        "                print(f\"[{config.run_name}] frame {frame_idx:7d} | eps={epsilon:.3f} | R={reward:6.1f} | mean100={mean_r:6.2f}\")\n",
        "\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, tgt_net, DEVICE, config.use_ddqn)\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    model_path = os.path.join(\"outputs\", f\"{config.run_name}_net.pth\")\n",
        "    torch.save(net.state_dict(), model_path)\n",
        "    print(f\"[{config.run_name}] Saved model to {model_path}\")\n",
        "\n",
        "    final_mean_reward = float(\"nan\")\n",
        "    if mean_rewards:\n",
        "        frames, means = zip(*mean_rewards)\n",
        "        final_mean_reward = float(means[-1])\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.plot(frames, means)\n",
        "        plt.xlabel(\"Frames\")\n",
        "        plt.ylabel(\"Mean reward (100 ep)\")\n",
        "        plt.grid(True)\n",
        "        curve_path = os.path.join(\"outputs\", f\"{config.run_name}_learning_curve.png\")\n",
        "        plt.savefig(curve_path, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    log_entry = {\n",
        "        \"run_name\": config.run_name,\n",
        "        \"variant_name\": config.variant_name,\n",
        "        \"use_ddqn\": config.use_ddqn,\n",
        "        \"max_frames\": config.max_frames,\n",
        "        \"eps_decay_frames\": config.eps_decay_frames,\n",
        "        \"gamma\": GAMMA,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"replay_size\": REPLAY_SIZE,\n",
        "        \"target_sync_frames\": SYNC_TARGET_FRAMES,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"final_mean_reward\": final_mean_reward,\n",
        "        \"comment\": config.comment,\n",
        "    }\n",
        "    experiment_logs.append(log_entry)\n",
        "\n",
        "    return model_path, final_mean_reward\n",
        "\n",
        "def record_video_random(env_id: str, filename: str, max_steps: int = VIDEO_MAX_STEPS):\n",
        "    env = make_env(env_id)\n",
        "    frames = []\n",
        "    state, _ = env.reset()\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.render())\n",
        "        a = env.action_space.sample()\n",
        "        state, _, terminated, truncated, _ = env.step(a)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    path = os.path.join(\"outputs\", filename)\n",
        "    imageio.mimsave(path, frames, fps=30)\n",
        "    print(f\"[VIDEO] Saved random video to {path}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def record_video_agent(env_id: str, model_path: str, filename: str, epsilon: float = 0.05, max_steps: int = VIDEO_MAX_STEPS):\n",
        "    env = make_env(env_id)\n",
        "    n_actions = env.action_space.n\n",
        "    obs_shape = env.observation_space.shape\n",
        "\n",
        "    net = DQN(obs_shape, n_actions).to(DEVICE)\n",
        "    net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    net.eval()\n",
        "\n",
        "    frames = []\n",
        "    state, _ = env.reset()\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.render())\n",
        "        if random.random() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            q = net(torch.as_tensor(state, device=DEVICE).unsqueeze(0))\n",
        "            a = int(q.argmax(dim=1).item())\n",
        "\n",
        "        state, _, terminated, truncated, _ = env.step(a)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    path = os.path.join(\"outputs\", filename)\n",
        "    imageio.mimsave(path, frames, fps=30)\n",
        "\n",
        "# experiment configs\n",
        "\n",
        "dqn_baseline_cfg = ExperimentConfig(\n",
        "    run_name=\"pong_dqn_baseline\",\n",
        "    variant_name=\"DQN\",\n",
        "    use_ddqn=False,\n",
        "    max_frames=MAX_FRAMES_DQN_BASE,\n",
        "    eps_decay_frames=EPS_DECAY_FRAMES_BASE,\n",
        "    comment=\"Baseline DQN\"\n",
        ")\n",
        "\n",
        "ddqn_baseline_cfg = ExperimentConfig(\n",
        "    run_name=\"pong_ddqn_baseline\",\n",
        "    variant_name=\"Double DQN\",\n",
        "    use_ddqn=True,\n",
        "    max_frames=MAX_FRAMES_DDQN_BASE,\n",
        "    eps_decay_frames=EPS_DECAY_FRAMES_BASE,\n",
        "    comment=\"Baseline DDQN\"\n",
        ")\n",
        "\n",
        "ddqn_tuned_cfg = ExperimentConfig(\n",
        "    run_name=\"pong_ddqn_tuned\",\n",
        "    variant_name=\"Double D\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfUrlKW5lLSO",
        "outputId": "28ec4e91-f22e-4eb3-b64e-28bafa188c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/434.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] Saved random video to outputs/pong_random.mp4\n",
            "\n",
            "=== Starting run: pong_dqn_baseline (DQN) ===\n",
            "  use_ddqn=False, max_frames=400000, eps_decay_frames=1000000\n",
            "  note: Baseline DQN from starter: long epsilon decay, 400k frames.\n",
            "[pong_dqn_baseline] frame    8857 | eps=0.991 | ep   10 | R= -21.0 | mean_100=-20.70\n",
            "[pong_dqn_baseline] frame   17862 | eps=0.982 | ep   20 | R= -19.0 | mean_100=-20.60\n",
            "[pong_dqn_baseline] frame   27917 | eps=0.972 | ep   30 | R= -20.0 | mean_100=-20.23\n",
            "[pong_dqn_baseline] frame   37576 | eps=0.962 | ep   40 | R= -20.0 | mean_100=-20.18\n",
            "[pong_dqn_baseline] frame   47106 | eps=0.953 | ep   50 | R= -21.0 | mean_100=-20.14\n",
            "[pong_dqn_baseline] frame   56568 | eps=0.943 | ep   60 | R= -21.0 | mean_100=-20.22\n",
            "[pong_dqn_baseline] frame   66169 | eps=0.934 | ep   70 | R= -20.0 | mean_100=-20.20\n",
            "[pong_dqn_baseline] frame   75492 | eps=0.925 | ep   80 | R= -21.0 | mean_100=-20.16\n",
            "[pong_dqn_baseline] frame   84732 | eps=0.915 | ep   90 | R= -20.0 | mean_100=-20.21\n",
            "[pong_dqn_baseline] frame   94653 | eps=0.905 | ep  100 | R= -21.0 | mean_100=-20.22\n",
            "[pong_dqn_baseline] frame  104712 | eps=0.895 | ep  110 | R= -21.0 | mean_100=-20.12\n",
            "[pong_dqn_baseline] frame  115703 | eps=0.884 | ep  120 | R= -20.0 | mean_100=-20.02\n",
            "[pong_dqn_baseline] frame  125972 | eps=0.874 | ep  130 | R= -17.0 | mean_100=-20.07\n",
            "[pong_dqn_baseline] frame  137064 | eps=0.863 | ep  140 | R= -20.0 | mean_100=-20.00\n",
            "[pong_dqn_baseline] frame  148210 | eps=0.852 | ep  150 | R= -20.0 | mean_100=-19.95\n",
            "[pong_dqn_baseline] frame  158307 | eps=0.842 | ep  160 | R= -20.0 | mean_100=-19.92\n",
            "[pong_dqn_baseline] frame  169210 | eps=0.831 | ep  170 | R= -21.0 | mean_100=-19.84\n",
            "[pong_dqn_baseline] frame  180671 | eps=0.819 | ep  180 | R= -20.0 | mean_100=-19.82\n",
            "[pong_dqn_baseline] frame  191745 | eps=0.808 | ep  190 | R= -17.0 | mean_100=-19.69\n",
            "[pong_dqn_baseline] frame  202680 | eps=0.797 | ep  200 | R= -20.0 | mean_100=-19.61\n",
            "[pong_dqn_baseline] frame  213431 | eps=0.787 | ep  210 | R= -20.0 | mean_100=-19.64\n",
            "[pong_dqn_baseline] frame  224810 | eps=0.775 | ep  220 | R= -19.0 | mean_100=-19.67\n",
            "[pong_dqn_baseline] frame  236809 | eps=0.763 | ep  230 | R= -16.0 | mean_100=-19.51\n",
            "[pong_dqn_baseline] frame  248698 | eps=0.751 | ep  240 | R= -19.0 | mean_100=-19.51\n",
            "[pong_dqn_baseline] frame  260181 | eps=0.740 | ep  250 | R= -19.0 | mean_100=-19.47\n",
            "[pong_dqn_baseline] frame  272556 | eps=0.727 | ep  260 | R= -20.0 | mean_100=-19.34\n",
            "[pong_dqn_baseline] frame  284092 | eps=0.716 | ep  270 | R= -21.0 | mean_100=-19.41\n",
            "[pong_dqn_baseline] frame  295859 | eps=0.704 | ep  280 | R= -19.0 | mean_100=-19.39\n",
            "[pong_dqn_baseline] frame  308803 | eps=0.691 | ep  290 | R= -19.0 | mean_100=-19.36\n",
            "[pong_dqn_baseline] frame  320920 | eps=0.679 | ep  300 | R= -19.0 | mean_100=-19.31\n",
            "[pong_dqn_baseline] frame  333790 | eps=0.666 | ep  310 | R= -19.0 | mean_100=-19.15\n",
            "[pong_dqn_baseline] frame  346739 | eps=0.653 | ep  320 | R= -18.0 | mean_100=-18.98\n",
            "[pong_dqn_baseline] frame  360946 | eps=0.639 | ep  330 | R= -18.0 | mean_100=-19.04\n",
            "[pong_dqn_baseline] frame  373327 | eps=0.627 | ep  340 | R= -20.0 | mean_100=-19.03\n",
            "[pong_dqn_baseline] frame  386685 | eps=0.613 | ep  350 | R= -18.0 | mean_100=-18.97\n",
            "[pong_dqn_baseline] Saved model to outputs/pong_dqn_baseline_net.pth\n",
            "[pong_dqn_baseline] Saved learning curve to outputs/pong_dqn_baseline_learning_curve.png\n",
            "\n",
            "=== Starting run: pong_ddqn_baseline (Double DQN) ===\n",
            "  use_ddqn=True, max_frames=400000, eps_decay_frames=1000000\n",
            "  note: Baseline DDQN: only change is DDQN target calculation.\n",
            "[pong_ddqn_baseline] frame    8801 | eps=0.991 | ep   10 | R= -21.0 | mean_100=-20.40\n",
            "[pong_ddqn_baseline] frame   18535 | eps=0.981 | ep   20 | R= -18.0 | mean_100=-20.10\n",
            "[pong_ddqn_baseline] frame   28062 | eps=0.972 | ep   30 | R= -21.0 | mean_100=-20.07\n",
            "[pong_ddqn_baseline] frame   36794 | eps=0.963 | ep   40 | R= -19.0 | mean_100=-20.20\n",
            "[pong_ddqn_baseline] frame   46203 | eps=0.954 | ep   50 | R= -20.0 | mean_100=-20.20\n",
            "[pong_ddqn_baseline] frame   55648 | eps=0.944 | ep   60 | R= -21.0 | mean_100=-20.23\n",
            "[pong_ddqn_baseline] frame   65284 | eps=0.935 | ep   70 | R= -21.0 | mean_100=-20.26\n",
            "[pong_ddqn_baseline] frame   74955 | eps=0.925 | ep   80 | R= -21.0 | mean_100=-20.26\n",
            "[pong_ddqn_baseline] frame   84632 | eps=0.915 | ep   90 | R= -21.0 | mean_100=-20.23\n",
            "[pong_ddqn_baseline] frame   94764 | eps=0.905 | ep  100 | R= -19.0 | mean_100=-20.18\n",
            "[pong_ddqn_baseline] frame  105234 | eps=0.895 | ep  110 | R= -20.0 | mean_100=-20.11\n",
            "[pong_ddqn_baseline] frame  115332 | eps=0.885 | ep  120 | R= -18.0 | mean_100=-20.13\n",
            "[pong_ddqn_baseline] frame  125419 | eps=0.875 | ep  130 | R= -21.0 | mean_100=-20.16\n",
            "[pong_ddqn_baseline] frame  134924 | eps=0.865 | ep  140 | R= -20.0 | mean_100=-20.14\n",
            "[pong_ddqn_baseline] frame  145809 | eps=0.854 | ep  150 | R= -20.0 | mean_100=-20.06\n",
            "[pong_ddqn_baseline] frame  156320 | eps=0.844 | ep  160 | R= -21.0 | mean_100=-19.97\n",
            "[pong_ddqn_baseline] frame  166213 | eps=0.834 | ep  170 | R= -20.0 | mean_100=-19.94\n",
            "[pong_ddqn_baseline] frame  176667 | eps=0.823 | ep  180 | R= -20.0 | mean_100=-19.83\n",
            "[pong_ddqn_baseline] frame  187567 | eps=0.812 | ep  190 | R= -18.0 | mean_100=-19.75\n",
            "[pong_ddqn_baseline] frame  198662 | eps=0.801 | ep  200 | R= -20.0 | mean_100=-19.75\n",
            "[pong_ddqn_baseline] frame  209541 | eps=0.790 | ep  210 | R= -20.0 | mean_100=-19.78\n",
            "[pong_ddqn_baseline] frame  222072 | eps=0.778 | ep  220 | R= -20.0 | mean_100=-19.64\n",
            "[pong_ddqn_baseline] frame  233039 | eps=0.767 | ep  230 | R= -20.0 | mean_100=-19.57\n",
            "[pong_ddqn_baseline] frame  244719 | eps=0.755 | ep  240 | R= -20.0 | mean_100=-19.45\n",
            "[pong_ddqn_baseline] frame  256234 | eps=0.744 | ep  250 | R= -17.0 | mean_100=-19.40\n",
            "[pong_ddqn_baseline] frame  267786 | eps=0.732 | ep  260 | R= -19.0 | mean_100=-19.33\n",
            "[pong_ddqn_baseline] frame  278684 | eps=0.721 | ep  270 | R= -19.0 | mean_100=-19.28\n",
            "[pong_ddqn_baseline] frame  289865 | eps=0.710 | ep  280 | R= -20.0 | mean_100=-19.30\n",
            "[pong_ddqn_baseline] frame  301427 | eps=0.699 | ep  290 | R= -20.0 | mean_100=-19.31\n",
            "[pong_ddqn_baseline] frame  314026 | eps=0.686 | ep  300 | R= -21.0 | mean_100=-19.17\n",
            "[pong_ddqn_baseline] frame  325676 | eps=0.674 | ep  310 | R= -20.0 | mean_100=-19.10\n",
            "[pong_ddqn_baseline] frame  339054 | eps=0.661 | ep  320 | R= -14.0 | mean_100=-19.08\n",
            "[pong_ddqn_baseline] frame  351345 | eps=0.649 | ep  330 | R= -21.0 | mean_100=-19.01\n",
            "[pong_ddqn_baseline] frame  364625 | eps=0.635 | ep  340 | R= -19.0 | mean_100=-18.91\n",
            "[pong_ddqn_baseline] frame  377566 | eps=0.622 | ep  350 | R= -19.0 | mean_100=-18.87\n",
            "[pong_ddqn_baseline] frame  390969 | eps=0.609 | ep  360 | R= -18.0 | mean_100=-18.81\n",
            "[pong_ddqn_baseline] Saved model to outputs/pong_ddqn_baseline_net.pth\n",
            "[pong_ddqn_baseline] Saved learning curve to outputs/pong_ddqn_baseline_learning_curve.png\n",
            "\n",
            "=== Starting run: pong_ddqn_tuned (Double DQN (tuned)) ===\n",
            "  use_ddqn=True, max_frames=600000, eps_decay_frames=300000\n",
            "  note: Tuned run: more frames (600k) and faster epsilon decay (300k) to encourage earlier exploitation.\n",
            "[pong_ddqn_tuned] frame    9101 | eps=0.970 | ep   10 | R= -20.0 | mean_100=-20.40\n",
            "[pong_ddqn_tuned] frame   18633 | eps=0.938 | ep   20 | R= -21.0 | mean_100=-20.35\n",
            "[pong_ddqn_tuned] frame   28065 | eps=0.906 | ep   30 | R= -21.0 | mean_100=-20.33\n",
            "[pong_ddqn_tuned] frame   38333 | eps=0.872 | ep   40 | R= -20.0 | mean_100=-20.23\n",
            "[pong_ddqn_tuned] frame   47667 | eps=0.841 | ep   50 | R= -18.0 | mean_100=-20.22\n",
            "[pong_ddqn_tuned] frame   57654 | eps=0.808 | ep   60 | R= -19.0 | mean_100=-20.15\n",
            "[pong_ddqn_tuned] frame   68293 | eps=0.772 | ep   70 | R= -18.0 | mean_100=-20.11\n",
            "[pong_ddqn_tuned] frame   79550 | eps=0.735 | ep   80 | R= -21.0 | mean_100=-20.09\n",
            "[pong_ddqn_tuned] frame   92212 | eps=0.693 | ep   90 | R= -18.0 | mean_100=-19.97\n",
            "[pong_ddqn_tuned] frame  104806 | eps=0.651 | ep  100 | R= -20.0 | mean_100=-19.83\n",
            "[pong_ddqn_tuned] frame  118790 | eps=0.604 | ep  110 | R= -17.0 | mean_100=-19.55\n",
            "[pong_ddqn_tuned] frame  133903 | eps=0.554 | ep  120 | R= -16.0 | mean_100=-19.27\n",
            "[pong_ddqn_tuned] frame  148881 | eps=0.504 | ep  130 | R= -17.0 | mean_100=-19.05\n",
            "[pong_ddqn_tuned] frame  165367 | eps=0.449 | ep  140 | R= -18.0 | mean_100=-18.78\n",
            "[pong_ddqn_tuned] frame  183899 | eps=0.387 | ep  150 | R= -12.0 | mean_100=-18.34\n",
            "[pong_ddqn_tuned] frame  203063 | eps=0.323 | ep  160 | R= -11.0 | mean_100=-17.86\n",
            "[pong_ddqn_tuned] frame  226033 | eps=0.247 | ep  170 | R= -15.0 | mean_100=-17.16\n",
            "[pong_ddqn_tuned] frame  248012 | eps=0.173 | ep  180 | R= -13.0 | mean_100=-16.57\n",
            "[pong_ddqn_tuned] frame  274131 | eps=0.086 | ep  190 | R= -12.0 | mean_100=-15.76\n",
            "[pong_ddqn_tuned] frame  310964 | eps=0.020 | ep  200 | R=  -3.0 | mean_100=-13.88\n",
            "[pong_ddqn_tuned] frame  344739 | eps=0.020 | ep  210 | R=   8.0 | mean_100=-11.54\n",
            "[pong_ddqn_tuned] frame  367642 | eps=0.020 | ep  220 | R=  17.0 | mean_100= -8.33\n",
            "[pong_ddqn_tuned] frame  387156 | eps=0.020 | ep  230 | R=  18.0 | mean_100= -4.71\n",
            "[pong_ddqn_tuned] frame  407391 | eps=0.020 | ep  240 | R=  18.0 | mean_100= -1.29\n",
            "[pong_ddqn_tuned] frame  426048 | eps=0.020 | ep  250 | R=  19.0 | mean_100=  2.17\n",
            "[pong_ddqn_tuned] frame  444216 | eps=0.020 | ep  260 | R=  21.0 | mean_100=  5.62\n",
            "[pong_ddqn_tuned] frame  462525 | eps=0.020 | ep  270 | R=  20.0 | mean_100=  8.81\n",
            "[pong_ddqn_tuned] frame  481877 | eps=0.020 | ep  280 | R=  19.0 | mean_100= 11.96\n",
            "[pong_ddqn_tuned] frame  500119 | eps=0.020 | ep  290 | R=  16.0 | mean_100= 14.96\n",
            "[pong_ddqn_tuned] frame  519171 | eps=0.020 | ep  300 | R=  21.0 | mean_100= 16.77\n",
            "[pong_ddqn_tuned] frame  537874 | eps=0.020 | ep  310 | R=  16.0 | mean_100= 18.02\n",
            "[pong_ddqn_tuned] frame  556983 | eps=0.020 | ep  320 | R=  19.0 | mean_100= 18.41\n",
            "[pong_ddqn_tuned] frame  575154 | eps=0.020 | ep  330 | R=  20.0 | mean_100= 18.54\n",
            "[pong_ddqn_tuned] frame  594762 | eps=0.020 | ep  340 | R=  18.0 | mean_100= 18.64\n",
            "[pong_ddqn_tuned] Saved model to outputs/pong_ddqn_tuned_net.pth\n",
            "[pong_ddqn_tuned] Saved learning curve to outputs/pong_ddqn_tuned_learning_curve.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] Saved agent video to outputs/pong_dqn_baseline.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] Saved agent video to outputs/pong_ddqn_baseline.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (160, 210) to (160, 224) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VIDEO] Saved agent video to outputs/pong_ddqn_tuned.mp4\n",
            "\n",
            "Saved experiment log to outputs/experiment_log.csv\n",
            "\n",
            "=== Final mean rewards (approx. over last 100 episodes) ===\n",
            "pong_dqn_baseline     : -18.89\n",
            "pong_ddqn_baseline    : -18.69\n",
            "pong_ddqn_tuned       :  18.64\n",
            "\n",
            "Done. Check the 'outputs/' folder in the Colab file browser.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('pong_project_outputs', 'zip', 'outputs')\n",
        "\n",
        "files.download('pong_project_outputs.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-KZZomLStTmS",
        "outputId": "c941b8ff-f6db-458d-deba-bca7f9b55a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping files...\n",
            "Downloading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8abb4155-205b-4e58-8cb6-fe66cec348d4\", \"pong_project_outputs.zip\", 19404899)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}